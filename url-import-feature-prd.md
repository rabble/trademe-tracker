# PRD: URL-Based Property Import Feature

This document outlines requirements for adding a **URL-based property import** feature to the existing MiVoy application. The feature will allow users to submit a property URL, which is then scraped (using AI/agent logic) to automatically populate a new listing in MiVoy's **existing data model**.

---

## 1. Purpose & Overview

- **Goal**: Enable users to quickly add properties to their MiVoy collections by simply pasting a URL from any external real estate or listing site.
- **Motivation**: Reduce manual data entry and grow MiVoy's database of discoverable properties.
- **Desired Outcome**: A single form field (Phase 1) where a user can paste a URL, prompting MiVoy's system to scrape relevant property info, fill out a new listing, and store it in our existing data model.

---

## 2. User Flow

1. **User inputs property URL** via a form in the MiVoy interface.
2. **System Scrapes** the page using an AI agent:
   - Extracts known property fields (title, images, price, location, description, etc.).
3. **Data Mapped to MiVoy's Existing Data Model**:
   - Populates fields in the new `Property` record (see [Section 3](#3-existing-data-model)).
4. **Review & Confirm**:
   - The user sees a pre-filled form with the scraped data.
   - The user can edit or confirm the info before saving.
5. **Save Listing**:
   - The new listing is added to the user's private collection **or** shared publicly, as chosen.
6. **Discoverability**:
   - If marked public, the listing is available on MiVoy's feed and for other users to explore.

---

## 3. Existing Data Model

Below is an example of the **existing `Property` data model** in MiVoy. The new feature will map scraped data into these fields (where available). (These fields already exist in the app's codebase.)

| Field            | Type       | Required | Notes                                                                      |
|------------------|------------|----------|----------------------------------------------------------------------------|
| `id`             | UUID       | Yes      | Unique identifier for each property (generated by MiVoy).                  |
| `userId`         | UUID       | Yes      | References the user who owns/created this property listing.               |
| `title`          | String     | Yes      | A short descriptive title of the property.                                 |
| `description`    | Text       | No       | Full text description.                                                     |
| `images`         | String[]   | No       | Array of image URLs for the property.                                      |
| `price`          | Number     | No       | Numeric price of the property.                                             |
| `currency`       | String     | No       | Currency code (e.g., USD, NZD, EUR).                                       |
| `location`       | String     | No       | Parsed location string (city, region, country).                            |
| `sourceUrl`      | String     | No       | **New**: The original URL from which data was scraped. (Might already exist, confirm or add.) |
| `visibility`     | String     | No       | Typically `public` or `private` (could be an enum).                        |
| `createdAt`      | DateTime   | Yes      | Auto-generated at property creation.                                       |
| `updatedAt`      | DateTime   | Yes      | Auto-updated when the property is modified.                                |

> **Note**: If `sourceUrl` is not already present in the model, we will add it so we can trace the listing back to its source.

---

## 4. Feature Requirements

1. **URL Input Form**
   - Single text field for the user to paste a valid URL (HTTP/HTTPS).
   - "Import" button triggers the scraping process.

2. **Scraping / AI Agent**
   - Fetches the HTML of the provided URL.
   - Extracts data such as:
     - **Title/Headline**
     - **Price** (numeric value and currency if available)
     - **Primary & Secondary Images**
     - **Location** (country, city, region)
     - **Property Description**  
   - Handles structured data if available (OpenGraph, JSON-LD) or attempts to parse page content.  
   - If scraping fails or data is incomplete, the system falls back to partial data or shows an error.

3. **User Review Screen**
   - Displays each scraped field (title, images, etc.) in editable form fields.
   - Allows user to correct or add missing info.
   - **Visibility** toggle: private vs. public.

4. **Persist to Database**
   - On "Save," the system creates or updates a `Property` record in the existing MiVoy data model.
   - Associates `userId` = current user.
   - Logs `sourceUrl` = the original input URL.
   - Ensures indexing for public discovery if `visibility = public`.

5. **Feedback & Validation**
   - Success message or error handling if scraping fails.
   - Possibly an in-app or email notification to confirm listing creation.

---

## 5. Acceptance Criteria

- [x] **User can input a URL** in a dedicated form.  
- [x] **System scrapes** the external site and retrieves at least a title, image(s), and any location/price data available.  
- [x] **Scraped data** is shown in an editable preview form before final save.  
- [ ] **Saved listing** is stored in the existing `Property` table with `sourceUrl`, linked to the user.  
- [ ] **Visibility** can be set to `private` or `public`. Public listings appear in MiVoy's discoverable feed.  
- [x] **Error Handling**: If scraping fails, user is given an error message and the option to manually enter details.  
- [x] **Checklist** generated (see [Section 7](#7-checklist-for-implementation)) for development tracking.

---

## 6. Future Enhancements

- **Browser Extension / Bookmarklet**: A one-click way to import properties directly from the listing site.  
- **Duplicate Detection**: Check if `sourceUrl` has already been imported by any user.  
- **Automated Tagging**: AI suggests property tags (e.g., "Beachfront," "Rural," "City Loft").  
- **Price Tracking**: Periodic re-scrape to detect price changes.  
- **Moderation / Reporting**: Mechanisms to handle spam or inappropriate listings.

---

## 7. Checklist for Implementation

Below is a **task checklist** to guide development and integration:

### 7.1 **Backend**
- [x] Create or confirm existence of `sourceUrl` field in the `Property` model.  
- [x] Implement AI/agent-based scraping service or module:
  - [x] Validate the provided URL.
  - [x] Fetch and parse the page.
  - [x] Map extracted data to relevant fields (title, description, price, images, location).
  - [x] Handle errors gracefully.
- [x] Integrate the scraping service with existing API endpoints (e.g., `/property/import`).

### 7.2 **Frontend**
- [x] **Import Form**:
  - [x] Single text input for URL.
  - [x] "Import" button that calls the scraping endpoint.
- [x] **Preview Screen**:
  - [x] Display editable fields pre-populated with scraped data.
  - [ ] Provide toggles for property visibility (private/public).
  - [x] "Save" button that submits final data to the backend.
- [x] **Error Handling UI**:
  - [x] Show friendly error messages if scraping fails.
  - [x] Option to manually input fields if no data was retrieved.

### 7.3 **Database & Migration**
- [x] Confirm if `sourceUrl` is already present:
  - [x] `url` field is already present in the Property model.
- [ ] Index `sourceUrl` if needed for duplicates or analytics.

### 7.4 **Testing**
- [x] **Unit Tests** for the scraping service (mock HTML inputs).  
- [ ] **Integration Tests** to ensure end-to-end flow (URL -> scraped -> user review -> property saved).  
- [x] **UI Tests** for form submission, error handling, and final listing creation.  
- [x] **Performance** check for scraping times (timeout or heavy pages).

### 7.5 **Documentation & Rollout**
- [x] Update **User Docs** to explain how to import a property by URL.  
- [ ] Announce feature in **Release Notes**.

---

## 8. Open Questions

1. **Page Access / Rate Limits**: How do we handle sites blocking our scrape? 
2. **Legal Considerations**: Some sites may disallow content scraping; do we need disclaimers or user responsibilities?
3. **Fallback**: Should we automatically prompt manual entry if the scrape is partial (e.g., images fail to load)?

---

## 9. Approvals

- **Product Lead**: *TBD*  
- **Engineering Lead**: *TBD*  
- **Design Lead**: *TBD*  
- **AI/Data Lead**: *TBD*  

**Sign-off** indicates agreement on scope, requirements, and the checklist tasks.

---

**End of Document**